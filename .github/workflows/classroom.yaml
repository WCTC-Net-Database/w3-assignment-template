name: Autograding

on:
  push:
  pull_request:
  workflow_dispatch:

jobs:
  grade:
    runs-on: ubuntu-latest
    permissions:
      checks: write
      contents: read

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: 8.0.x

      - name: Restore
        run: dotnet restore

      - name: Build
        run: dotnet build --configuration Release --no-restore

      - name: Test (produce TRX)
        run: dotnet test --configuration Release --no-build --logger "trx;LogFileName=results.trx"

      - name: Ensure jq (for JSON validation)
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Compute grade JSON (namespace-safe)
        shell: bash
        run: |
          set -euo pipefail

          # Find the test results file
          trx="$(find . -type f -name results.trx | head -n1)"
          [[ -z "$trx" ]] && { echo "No results.trx found"; exit 1; }

          # Write a tiny Python parser; NOTE: the closing 'PY' must be at column 0
          cat > parse_trx.py <<'PY'
import sys, json, xml.etree.ElementTree as ET
trx_path = sys.argv[1]
root = ET.parse(trx_path).getroot()

# Locate <Counters> regardless of namespace
counters = None
for elem in root.iter():
    if elem.tag.rsplit('}', 1)[-1] == 'Counters':
        counters = elem
        break

if counters is None:
    print("Could not find <Counters> in TRX", file=sys.stderr)
    sys.exit(1)

def geti(name):
    v = counters.get(name)
    try:
        return int(v)
    except (TypeError, ValueError):
        return 0

total  = geti('total')
passed = geti('passed')
status = "ok" if (total > 0 and passed == total) else "failed"

out = {
  "version": 1,
  "status": status,
  "tests": [{
      "name": "Unit tests",
      "number": 1,
      "score": passed,
      "max_score": total,
      "output": f"Passed {passed} of {total} unit tests."
  }]
}
print(json.dumps(out))
PY

          # Run the parser and validate JSON
          python3 parse_trx.py "$trx" > unit.json
          echo "Debug unit.json:"; cat unit.json
          jq -e . unit.json >/dev/null

          # Export results to env (multiline-safe block)
          {
            echo "UNIT_RESULTS<<EOF"
            cat unit.json
            echo "EOF"
          } >> "$GITHUB_ENV"

          # Clean up the temp script
          rm -f parse_trx.py

      - name: Upload test results artifact
        uses: actions/upload-artifact@v4
        with:
          name: test-results
          path: "**/results.trx"

      - name: Publish grade to Checks
        if: always()
        uses: actions/github-script@v7
        env:
          RESULTS: ${{ env.UNIT_RESULTS }}
        with:
          script: |
            const results = JSON.parse(process.env.RESULTS || "{}");
            const t = (results.tests && results.tests[0]) || {};
            const points = Number.isFinite(t.score) ? t.score : 0;
            const max    = Number.isFinite(t.max_score) ? t.max_score : 0;
            const summary = `Points ${points}/${max}`;
            await github.rest.checks.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'Autograding',
              head_sha: context.sha,
              status: 'completed',
              conclusion: (points === max && max > 0) ? 'success' : 'failure',
              output: {
                title: 'Autograding',
                summary,
                text: t.output || ''
              }
            });

      # If you later want to use the Classroom reporter instead of the Checks step above,
      # comment out "Publish grade to Checks" and uncomment the block below.
      #
      # - name: Report grades to GitHub Classroom
      #   if: ${{ always() && env.UNIT_RESULTS != '' }}
      #   uses: classroom-resources/autograding-grading-reporter@v1
      #   with:
      #     runners: UNIT
      #     token: ${{ secrets.GITHUB_TOKEN }}
